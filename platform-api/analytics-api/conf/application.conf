
# This is the main configuration file for the application.
# ~~~~~

# Secret key
# ~~~~~
# The secret key is used to secure cryptographics functions.
# If you deploy your application to several instances be sure to use the same key!
application.secret="DFHdR74?tDvH2n`DAqk:RBl6NfkoO5tNYblTRmf3ZLcDIp5@oVjJM^ypkDOf2`<A"


# The application languages
# ~~~~~
application.langs="en"

# Global object class
# ~~~~~
# Define the Global object class for this application.
# Default to Global in the root package.
# application.global=Global

# Router
# ~~~~~
# Define the Router object to use for this application.
# This router will be looked up first when the application is starting up,
# so make sure this is the entry point.
# Furthermore, it's assumed your route file is named properly.
# So for an application router like `my.application.Router`,
# you may need to define a router file `conf/my.application.routes`.
# Default to Routes in the root package (and conf/routes)
# application.router=my.application.Routes

# Database configuration
# ~~~~~
# You can declare as many datasources as you want.
# By convention, the default datasource is named `default`
#
# db.default.driver=org.h2.Driver
# db.default.url="jdbc:h2:mem:play"
# db.default.user=sa
# db.default.password=""

# Evolutions
# ~~~~~
# You can disable evolutions if needed
# evolutionplugin=disabled

# Logger
# ~~~~~
# You can also configure logback (http://logback.qos.ch/),
# by providing an application-logger.xml file in the conf directory.

# Root logger:
logger.root=ERROR

# Logger used by the framework:
logger.play=INFO

# Logger provided to your application:
logger.application=DEBUG


# Test Configurations
cassandra.service.embedded.enable=false
cassandra.cql_path="../../platform-scripts/database/data.cql"
cassandra.service.embedded.connection.port=9142

spark.cassandra.connection.host="127.0.0.1"
cassandra.keyspace_prefix="local"
cassandra.hierarchy_store_prefix="dev_"

# Content to vec configurations
content2vec.content_service_url="https://dev.ekstep.in/api/learning"
recommendation.train_model_job="src/test/resources/run-job.sh device-recos &"


# Recommendations configurations
service.search.url="https://dev.open-sunbird.org/api/composite"
service.search.path="/v1/search"
service.search.requestbody="""{"request":{"filters":{"objectType":["Content"],"contentType":["Story","Worksheet","Collection","Game"],"status":["Live"]},"limit":1000}}"""
service.search.limit="10"

recommendation.enable=true
recommendation.limit="10"
recommendation.surprise_find.enable=true
recommendation.surprise_find.index="15"
recommendation.surprise_find.serendipity_factor="20"


dataproduct.scripts_path=src/test/resources

# Metrics API configuration
#metrics.search.type="s3"
#metrics.search.params={"bucket":"ekstep-dev-data-store", "path":"metrics/"}
metrics.search.type="local"
metrics.search.params={"bucket":"", "path":"test/resources/metrics/"}
metrics.period.format.day="MMM dd EEE"
metrics.period.format.month="MMM YYYY"
metrics.period.format.year="YYYY"


# Data Exhaust API
data_exhaust.list.limit="100"
data_exhaust.retry.limit="3"
data_exhaust.dataset.list=["eks-consumption-raw", "eks-consumption-summary", "eks-consumption-metrics","eks-creation-raw", "eks-creation-summary", "eks-creation-metrics"]
data_exhaust.dataset.default="eks-consumption-raw"
data_exhaust.output_format="json"



# Log4j Kafka appender config
log4j.appender.kafka.enable="false"
log4j.appender.kafka.broker_host="localhost:9092"
log4j.appender.kafka.topic="sandbox.telemetry.backend"

device-register-controller {
  type = "Dispatcher"
  executor = "fork-join-executor"
  fork-join-executor {
    # The parallelism factor is used to determine thread pool size using the
    # following formula: ceil(available processors * factor). Resulting size
    # is then bounded by the parallelism-min and parallelism-max values.
    parallelism-factor = 3.0

    # Min number of threads to cap factor-based parallelism number to
    parallelism-min = 8

    # Max number of threads to cap factor-based parallelism number to
    parallelism-max = 16
  }
  # Throughput for default Dispatcher, set to 1 for as fair as possible
  throughput = 1
}

device-register-actor {
  type = "Dispatcher"
  executor = "fork-join-executor"
  fork-join-executor {
    # The parallelism factor is used to determine thread pool size using the
    # following formula: ceil(available processors * factor). Resulting size
    # is then bounded by the parallelism-min and parallelism-max values.
    parallelism-factor = 3.0

    # Min number of threads to cap factor-based parallelism number to
    parallelism-min = 8

    # Max number of threads to cap factor-based parallelism number to
    parallelism-max = 16
  }
  # Throughput for default Dispatcher, set to 1 for as fair as possible
  throughput = 1
}

default-dispatcher {
  executor = "fork-join-executor"
  fork-join-executor {
    # The parallelism factor is used to determine thread pool size using the
    # following formula: ceil(available processors * factor). Resulting size
    # is then bounded by the parallelism-min and parallelism-max values.
    parallelism-factor = 3.0

    # Min number of threads to cap factor-based parallelism number to
    parallelism-min = 8

    # Max number of threads to cap factor-based parallelism number to
    parallelism-max = 16
  }
  # Throughput for default Dispatcher, set to 1 for as fair as possible
  throughput = 1
}

#AKKA Configuration
akka {
  actor {
  	deployment {

        /metricsApiActor {
          router = smallest-mailbox-pool
          nr-of-instances = 10
        }
        /jobApiActor {
          router = smallest-mailbox-pool
          nr-of-instances = 10
        }
        /recommendAPIActor {
          router = smallest-mailbox-pool
          nr-of-instances = 10
        }
        /healthCheckAPIActor {
          router = smallest-mailbox-pool
          nr-of-instances = 10
        }
        /tagServiceAPIActor {
        	router = smallest-mailbox-pool
          nr-of-instances = 10
        }
        /deviceRegisterServiceAPIActor {
        	router = smallest-mailbox-pool
          nr-of-instances = 10
          dispatcher = device-register-dispatcher
        }
    }
  }
}

#Netty Configuration
play.server {

  # The server provider class name
  provider = "play.core.server.NettyServerProvider"

  netty {

    # The number of event loop threads. 0 means let Netty decide, which by default will select 2 times the number of
    # available processors.
    eventLoopThreads = 30

    # Whether the Netty wire should be logged
    log.wire = true
    
    # The transport to use, either jdk or native.
    # Native socket transport has higher performance and produces less garbage but are only available on linux 
    transport = "native"
  }
}

# play.modules.enabled+="MetricsModule"
#play.modules.enabled+="com.kenshoo.play.metrics.PlayModule"

# app & channel id

default.consumption.app.id="no_value"
default.channel.id="in.ekstep"
default.creation.app.id="no_value"

elasticsearch.service.endpoint="http://localhost:9200"
elasticsearch.index.compositesearch.name="compositesearch"
elasticsearch.index.dialcodemetrics.name="dialcodemetrics"
metrics.dialcodemetrics.request.limit=1000

org.search.api.url="https://dev.sunbirded.org/api/org/v1/search"
org.search.api.key="org-search-api-key"

postgres.db="analytics"
postgres.url="jdbc:postgresql://localhost:5432/"
postgres.user="analytics"
postgres.pass="analytics"
postgres.table_name="consumer_channel_mapping"
postgres.table.geo_location_city.name="geo_location_city"
postgres.table.geo_location_city_ipv4.name="geo_location_city_ipv4"

default.channel="in.ekstep"

channel.data_exhaust.bucket="ekstep-dev-data-store"
channel.data_exhaust.basePrefix="channel-exhaust/"
channel.data_exhaust.expiryMins=30
dataexhaust.authorization_check=true

storage-service.request-signature-version="AWS4-HMAC-SHA256"
s3service.region="ap-south-1"

application.env="local"
