application.env="local"
telemetry.version="2.1"
default.parallelization=10
spark_output_temp_dir="/tmp/"

# the below urls should be https - do not update them
lp.url="https://dev.ekstep.in/api/learning"
lp.path="/api/learning/v2/content/"


# Content to vec configurations
content2vec_scripts_path="../../platform-scripts/python/main/vidyavaani"
pc_files_cache="file"
pc_dispatch_params="""{"bucket":""}"""
pc_files_prefix="/tmp/data-sets/"

metrics.dispatch.to="file"
metrics.dispatch.params="""{"bucket":""}"""
metrics.consumption.dataset.id="/tmp/data-sets/eks-consumption-metrics/"
metrics.creation.dataset.id="/tmp/data-sets/eks-creation-metrics/"

default.consumption.app.id="genie"
default.channel.id="in.ekstep"
default.creation.app.id="portal"

lp.contentmodel.versionkey=jd5ECm/o0BXwQCe8PfZY1NoUkB9HN41QjA80p22MKyRIcP5RW4qHw8sZztCzv87M

reactiveinflux {
  url = "http://localhost:8086/"
  spark {
    batchSize = 1000
  }
  database = "business_metrics_test_db"
  awaitatmost = "60"
}

# Neo4j
neo4j.bolt.url="bolt://localhost:7687"
neo4j.bolt.user="neo4j"
neo4j.bolt.password="@n@lytic5"


# Test Configurations
graph.service.embedded.enable=true
graph.service.embedded.dbpath="/tmp/graph.db/"
graph.content.limit="100"
cassandra.service.embedded.enable=false
cassandra.cql_path="../../platform-scripts/database/data.cql"
cassandra.service.embedded.connection.port=9142
cassandra.keyspace_prefix="local_"
cassandra.hierarchy_store_prefix="dev_"

spark.cassandra.connection.host=127.0.0.1

# Slack Configurations
monitor.notification.channel = "testing"
monitor.notification.name = "dp-monitor"
monitor.notification.slack = false

# DataExhaust configuration
data_exhaust {
	save_config {
		save_type="local"
		bucket="ekstep-dev-data-store"
		prefix="/tmp/data-exhaust/"
		public_s3_url="s3://"
		local_path="/tmp/data-exhaust-package/"
	}
	delete_source: "false"
	package.enable: "false"
}

cloud_storage_type="azure"

service.search.url="https://dev.open-sunbird.org/api/composite"
service.search.path="/v1/search"

# course metrics container in azure
es.host="http://localhost"
es.port="9200"
course.metrics.es.alias="cbatchstats"
course.metrics.es.index.cbatchstats.prefix="cbatchstats-"
course.metrics.es.index.cbatch="cbatch"
course.metrics.cassandra.sunbirdKeyspace="sunbird"
course.metrics.cassandra.sunbirdCoursesKeyspace="sunbird_courses"
course.metrics.cloud.provider="azure"
course.metrics.cloud.container="course-metrics"
course.metrics.cloud.objectKey="reports/"
course.metrics.temp.dir="/Users/sunil/course-reports"
course.metrics.cassandra.input.consistency="QUORUM"
# for only testing uploads to blob store
azure_storage_key=""
azure_storage_secret=""

# content rating configurations
druid.sql.host="http://11.2.1.20:8082/druid/v2/sql/"
druid.unique.content.query="{\"query\":\"SELECT DISTINCT \\\"object_id\\\" AS \\\"Id\\\"\\nFROM \\\"druid\\\".\\\"telemetry-events\\\" WHERE \\\"eid\\\" = 'FEEDBACK' AND \\\"__time\\\" BETWEEN TIMESTAMP '%s' AND TIMESTAMP '%s' \"}"
druid.content.rating.query="{\"query\": \"SELECT \\\"object_id\\\" AS ContentId, COUNT(*) AS \\\"Number of Ratings\\\", SUM(edata_rating) AS \\\"Total Ratings\\\", SUM(edata_rating)/COUNT(*) AS \\\"AverageRating\\\" FROM \\\"druid\\\".\\\"telemetry-events\\\" WHERE \\\"eid\\\" = 'FEEDBACK' GROUP BY \\\"object_id\\\"\"}"
lp.system.update.base.url="http://localhost:8080/learning-service/system/v3/content/update"

#Experiment Configuration
user.search.api.url = "http://localhost:9000/private/user/v1/search"
user.search.limit = 10000

spark.memory_fraction=0.3
spark.storage_fraction=0.5
spark.driver_memory=1g

# pipeline auditing
druid.pipeline_metrics.audit.query="{\"query\":\"SELECT \\\"job-name\\\", SUM(\\\"success-message-count\\\") AS \\\"success-message-count\\\", SUM(\\\"failed-message-count\\\") AS \\\"failed-message-count\\\", SUM(\\\"duplicate-event-count\\\") AS \\\"duplicate-event-count\\\", SUM(\\\"batch-success-count\\\") AS \\\"batch-success-count\\\", SUM(\\\"batch-error-count\\\") AS \\\"batch-error-count\\\", SUM(\\\"primary-route-success-count\\\") AS \\\"primary-route-success-count\\\", SUM(\\\"secondary-route-success-count\\\") AS \\\"secondary-route-success-count\\\" FROM \\\"druid\\\".\\\"pipeline-metrics\\\" WHERE \\\"job-name\\\" IN (%s) AND \\\"__time\\\" BETWEEN TIMESTAMP '%s' AND TIMESTAMP '%s' GROUP BY \\\"job-name\\\" \"}"
druid.datasource.count.query="{ \"query\": \"SELECT COUNT(*) AS \\\"total\\\" FROM \\\"druid\\\".\\\"%s\\\" WHERE TIME_FORMAT(MILLIS_TO_TIMESTAMP(\\\"syncts\\\"), 'yyyy-MM-dd HH:mm:ss.SSS', 'Asia/Kolkata') BETWEEN TIMESTAMP '%s' AND '%s' AND  \\\"__time\\\" BETWEEN TIMESTAMP '%s' AND TIMESTAMP '%s'\" }"